from pathlib import Path
import argparse
import json
from coder.utils import norm_edit_similarity
from transformers import GPT2TokenizerFast

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--results', type=str, required=True, help='Path to results directory generated by process_results.py')
    args = parser.parse_args()
    here = Path(__file__).parent.resolve()
    res_dir = Path(args.results).resolve()
    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
    with open(res_dir/'complete.json', 'r') as f:
        results = json.load(f)
    token_similarity = []
    for mode, sub_res in results.items():
        if mode.startswith('baseline'):
            baseline = mode
            continue
        for id, res in sub_res.items():
            if id == 0:
                continue
            with open(res_dir/f'gt-{id}.md') as f:
                gt = f.read().split('\nground truth:\n```python\n')[1].split('\n```')[0].replace('\n', ' ')
            completions = []
            with open(res_dir/f'{mode}-{id}.md') as f:
                    content = f.read()
            for k in range(len(res)):
                completions.append(content.split(f'\ncompletion {k}:\n```python\n')[1].split('\n```')[0].replace('\n', ' '))
            token_similarity.append([])
            gt_tokens = tokenizer(gt)['input_ids']
            for k in range(len(res)):
                token_similarity[-1].append(norm_edit_similarity(gt_tokens, tokenizer(completions[k])['input_ids']))
    avg = []
    best = []
    for j in range(len(token_similarity[0])):
        avg.append(sum([x[j] for x in token_similarity]) / len(token_similarity))
        best.append(sum([max(x[:j+1]) for x in token_similarity]) / len(token_similarity))
    print(f'{args.results} ' + ' '.join([str(x) for x in avg]) + ' ' + ' '.join([str(x) for x in best]))
